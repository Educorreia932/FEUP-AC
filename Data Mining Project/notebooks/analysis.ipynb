{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "train_df = pd.read_pickle(\"../out/train.pkl\")\n",
    "test_df = pd.read_pickle(\"../out/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client count.\n",
    "In both the training and test datasets, each account with a loan only has 1 or 2 clients. Therefore, for this analysis, it will be counted as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encountered client count values in the training dataset:\\n\" + str(train_df[\"client_count_mean\"].value_counts().index.to_list()) + \" \\nTesting dataset:\\n\" + str(test_df[\"client_count_mean\"].value_counts().index.to_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove categoric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_analysis = deepcopy(train_df)\n",
    "\n",
    "# # Removing ids\n",
    "# del train_df_analysis[\"loan_id\"]\n",
    "# Removing categoric variables\n",
    "del train_df_analysis[\"status\"]\n",
    "#del train_df_analysis[\"sex\"]\n",
    "del train_df_analysis[\"num_times_under_zero\"]\n",
    "\n",
    "del train_df_analysis[\"issuance_frequency_per_month\"]\n",
    "# Also counts as categoric data for this analysis\n",
    "del train_df_analysis[\"client_count_mean\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Value distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 4\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=[12, 12])\n",
    "i = 0\n",
    "for col_name in train_df_analysis:\n",
    "    if(col_name != \"loan_id\"):\n",
    "        axs = sb.histplot(data=train_df_analysis, x=train_df_analysis[col_name], ax=ax[(int(i)%nrows), (int(i)//nrows)])\n",
    "        i = i + 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that may have outliers from looking at the histogram (by descending order of likelihood): \n",
    "balance_min, \n",
    "no. of municipalities with inhabitants < 499, \n",
    "days_since_last_transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 3\n",
    "ncols = 4\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=[12, 12])\n",
    "i = 0\n",
    "for col_name in train_df_analysis:\n",
    "    if(col_name != \"loan_id\"):\n",
    "        axs = sb.boxplot(data=train_df_analysis, x=train_df_analysis[col_name], ax=ax[(int(i)%nrows), (int(i)//nrows)])\n",
    "        i = i + 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables that may have outliers from looking at the boxplot (by descending order of likelihood): \n",
    "loan_amount, \n",
    "operation_count, \n",
    "days_since_last_transaction, \n",
    "no. of municipalities with inhabitants < 499, \n",
    "balance_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using coeficients of variation to compare each variable's deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeficients_of_variation = {}\n",
    "standard_deviations = {}\n",
    "means = {}\n",
    "# Calculate coeficients of variation (standard variation / mean * 100[%])\n",
    "for col_name in train_df_analysis:\n",
    "    coeficients_of_variation[col_name] = np.std(train_df_analysis[col_name])\n",
    "    standard_deviations[col_name] = coeficients_of_variation[col_name]\n",
    "    means[col_name] = np.average(train_df_analysis[col_name])\n",
    "    coeficients_of_variation[col_name] /= means[col_name] / 100\n",
    "coeficients_of_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of coeficients of variation\n",
    "coeficients_of_variation_sorted = dict(sorted(coeficients_of_variation.items(), key=lambda item: -item[1]))\n",
    "\n",
    "num_items = len(coeficients_of_variation_sorted.items())\n",
    "plt.bar(np.arange(num_items), coeficients_of_variation_sorted.values())\n",
    "plt.xticks(np.arange(num_items), coeficients_of_variation_sorted.keys(), rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliar detection methods\n",
    "### Standard deviation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 2-3 is normal for datasets of this size. This will not work for certain data, such as region data, since it has less values. For those, we will use the quartile method.\n",
    "std_multiplier = 3\n",
    "labels = train_df_analysis.columns.difference(train_df.columns.difference([\"loan_amount\", \"operation_count\", \"type_transaction_count_withdrawal\", \"days_since_last_transaction\"]))\n",
    "train_df_std_method = train_df#train_df_analysis.drop( axis=1, labels=train_df_analysis.columns.difference([\"loan_amount\", \"operation_count\", \"type_transaction_count_withdrawal\", \"no. of municipalities with inhabitants < 499\", \"days_since_last_transaction\"]))\n",
    "outliers = {}\n",
    "for col_name in labels:\n",
    "    low_bound, high_bound = means[col_name] - (standard_deviations[col_name] * std_multiplier), means[col_name] + (standard_deviations[col_name] * std_multiplier)\n",
    "    outliers[col_name] = train_df.loc[(train_df[col_name] < low_bound) | (train_df[col_name] > high_bound)]\n",
    "    #print(\"Name: \" + str(col_name) + \" Count: \" + str(outliers[col_name].count()) + \"Head10: \" + str(outliers[col_name].head(10)))\n",
    "\n",
    "for col_name in outliers:\n",
    "    if(outliers[col_name].any(axis=None)):\n",
    "        print(\"Name = \" + col_name + \":\")\n",
    "        display(outliers[col_name])\n",
    "        print(\"Std: \" + str(standard_deviations[col_name]))\n",
    "        print(\"Quartile 75s: \\n\")\n",
    "        for col_name in train_df_analysis:\n",
    "            print(str(col_name) + \" = \" + str(train_df[col_name].quantile(.75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using first/third quartile method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 2-3 is normal for datasets of this size. This will not work for certain data, such as region data, since it has less values. For those, we will use the quartile method.\n",
    "iqr_multiplier = 1.5\n",
    "labels = train_df_analysis.columns.difference(train_df.columns.difference([\"no. of municipalities with inhabitants < 499\",\"region_woe\"]))\n",
    "train_df_iqr_method = train_df\n",
    "outliers = {}\n",
    "for col_name in labels:\n",
    "    q1, q3 = train_df_iqr_method[col_name].quantile(.25), train_df_iqr_method[col_name].quantile(.75)\n",
    "    iqr = q3 - q1\n",
    "    low_bound, high_bound = q1 - iqr_multiplier * iqr, q3 + iqr_multiplier * iqr\n",
    "    outliers[col_name] = train_df.loc[(train_df[col_name] < low_bound) | (train_df[col_name] > high_bound)]\n",
    "\n",
    "for col_name in outliers:\n",
    "    if(outliers[col_name].any(axis=None)):\n",
    "        print(\"Name = \" + col_name + \":\")\n",
    "        display(outliers[col_name])\n",
    "        print(\"Quartile 75s: \\n\")\n",
    "        for col_name in train_df_analysis:\n",
    "            print(str(col_name) + \" = \" + str(train_df[col_name].quantile(.75)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df_analysis[\"loan_id\"]\n",
    "sb.pairplot(train_df_analysis)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3421c22e82f1f7adbe80b5f447de79011f606547c5b82031ed54be68ff924ec5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
