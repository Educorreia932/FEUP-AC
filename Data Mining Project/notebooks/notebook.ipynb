{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from copy import deepcopy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "account_df = read_to_df(\"account.csv\")\n",
    "card_test_df = read_to_df(\"card_test.csv\")\n",
    "card_train_df = read_to_df(\"card_train.csv\")\n",
    "client_df = read_to_df(\"client.csv\")\n",
    "disp_df = read_to_df(\"disp.csv\")\n",
    "district_df = read_to_df(\"district.csv\")\n",
    "loan_test_df = read_to_df(\"loan_test.csv\")\n",
    "loan_train_df = read_to_df(\"loan_train.csv\")\n",
    "trans_test_df = read_to_df(\"trans_test.csv\")\n",
    "trans_train_df = read_to_df(\"trans_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_transactions_per_week = 3\n",
    "avg_weeks_per_month = (365.25 / 7 / 12)\n",
    "\n",
    "account_df['frequency'] = account_df['frequency'].apply(lambda x: 1 if x == 'monthly issuance' else avg_weeks_per_month if x == 'weekly issuance' else (365.25 / 7 / 12) * avg_transactions_per_week)\n",
    "account_df[\"date\"] = account_df[\"date\"].apply(lambda x: read_date(x))\n",
    "account_df.rename(columns={\"date\": \"creation_date\", \"frequency\": \"issuance_frequency_per_month\"}, inplace=True)\n",
    "\n",
    "account_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process client data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_df[\"sex\"] = client_df[\"birth_number\"].apply(lambda x: 0 if int(str(x)[2:4]) > 50 else 1)\n",
    "client_df[\"age\"] = client_df[\"birth_number\"].apply(lambda x: calculate_age(read_date(x)))\n",
    "\n",
    "client_df.drop(\"birth_number\", inplace=True, axis=1)\n",
    "\n",
    "client_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process disposition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_df.rename(columns={\"type\": \"is_owner\"}, inplace=True)\n",
    "disp_df[\"is_owner\"].replace({\"OWNER\": True, \"DISPONENT\": False}, inplace=True)\n",
    "\n",
    "# Count number clients per account\n",
    "client_count_df = disp_df.groupby(\"account_id\", as_index=False, group_keys=False).agg(client_count=(\"is_owner\", \"count\"))\n",
    "\n",
    "disp_df = disp_df.merge(client_count_df, on=\"account_id\")\n",
    "disp_df = disp_df[disp_df[\"is_owner\"] == True] \n",
    "disp_df.drop(\"is_owner\", axis=1, inplace=True)\n",
    "\n",
    "disp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [trans_train_df, trans_test_df]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i][\"operation\"].replace(\n",
    "        {\n",
    "            \"credit in cash\": 1,\n",
    "            \"collection from another bank\": 2,\n",
    "            \"withdrawal in cash\": 3,\n",
    "            \"remittance to another bank\": 4,\n",
    "            \"credit card withdrawal\": 5,\n",
    "            \"interest credited\": 6\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    # Convert \"withdrawal in cash\" to \"withdrawal\" in type column\n",
    "    dataframes[i].loc[dataframes[i][\"type\"] == \"withdrawal in cash\", \"type\"] = \"withdrawal\"\n",
    "\n",
    "    # Withdrawal amounts should be negative\n",
    "    dataframes[i].loc[dataframes[i][\"type\"] == \"withdrawal\", \"amount\"] *= -1\n",
    "\n",
    "    dataframes[i][\"date\"] = dataframes[i][\"date\"].apply(lambda x: read_date(x))\n",
    "    dataframes[i].rename(columns={\"date\": \"transaction_date\"}, inplace=True)\n",
    "\n",
    "    dataframes[i].drop([\"k_symbol\", \"bank\", \"account\"], axis=1, inplace=True)\n",
    "\n",
    "trans_train_df, trans_test_df = dataframes\n",
    "\n",
    "trans_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [card_train_df, card_test_df]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i][\"type\"].replace({\"classic\": 1, \"junior\": 2, \"gold\": 3}, inplace=True)\n",
    "    dataframes[i][\"issued\"] = dataframes[i][\"issued\"].apply(lambda x: read_date(x))\n",
    "\n",
    "card_train_df, card_test_df = dataframes\n",
    "\n",
    "card_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "district_df.replace(\"?\", np.NaN, inplace=True)\n",
    "\n",
    "district_df[\"unemploymant rate '95\"].fillna(district_df[\"unemploymant rate '95\"].median(), inplace=True)\n",
    "district_df[\"no. of commited crimes '95\"].fillna(district_df[\"no. of commited crimes '95\"].median(), inplace=True)\n",
    "\n",
    "district_df[\"unemploymant rate '95\"] = pd.to_numeric(district_df[\"unemploymant rate '95\"])\n",
    "district_df[\"no. of commited crimes '95\"] = pd.to_numeric(district_df[\"no. of commited crimes '95\"])\n",
    "\n",
    "district_df[\"criminality_growth\"] = (district_df[\"no. of commited crimes '96\"] - district_df[\"no. of commited crimes '95\"]) / district_df[\"no. of inhabitants\"]\n",
    "district_df[\"unemployment_growth\"] = (district_df[\"unemploymant rate '96\"] - district_df[\"unemploymant rate '95\"])\n",
    "district_df[\"ratio_entrepeneurs\"] = district_df[\"no. of enterpreneurs per 1000 inhabitants\"] / 1000\n",
    "\n",
    "district_df.drop([\n",
    "    \"unemploymant rate '95\",\n",
    "    \"unemploymant rate '96\",\n",
    "    \"no. of commited crimes '95\",\n",
    "    \"no. of commited crimes '96\",\n",
    "    \"no. of enterpreneurs per 1000 inhabitants\"\n",
    "], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process loan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_dfs = [loan_train_df, loan_test_df]\n",
    "\n",
    "for i in range(len(loan_dfs)):\n",
    "    loan_dfs[i][\"date\"] = loan_dfs[i][\"date\"].apply(lambda x: read_date(x))\n",
    "    loan_dfs[i].rename(columns={\"date\": \"loan_date\", \"amount\": \"loan_amount\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate feature from transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions = (trans_train_df, trans_test_df)\n",
    "# account_features = [1, 2]\n",
    "\n",
    "# for i in range(len(transactions)):\n",
    "#     # Sorting transactions by date to figure out the most recent balance\n",
    "#     account_features[i] = transactions[i].sort_values(by=\"date\", axis=0, ascending=False)\n",
    "#     account_features[i].drop_duplicates(subset='account_id', keep='first', inplace=True)\n",
    "\n",
    "#     account_features[i].drop(account_features[i].columns.difference(['account_id', 'balance']), axis=1, inplace=True)\n",
    "#     account_features[i].rename(columns={'balance': 'final_amount'}, inplace=True)\n",
    "\n",
    "# account_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_dfs = [loan_train_df, loan_test_df]\n",
    "trans_dfs = (trans_train_df, trans_test_df)\n",
    "cards_dfs = (card_train_df, card_test_df)\n",
    "\n",
    "for i in range(len(loan_dfs)):\n",
    "    # Merge with dispositions\n",
    "    loan_dfs[i] = loan_dfs[i].merge(disp_df, on=\"account_id\", how=\"left\")\n",
    "\n",
    "    # Merge with accounts\n",
    "    loan_dfs[i] = loan_dfs[i].merge(account_df, on=\"account_id\")\n",
    "\n",
    "    # Merge with clients\n",
    "    loan_dfs[i] = loan_dfs[i].merge(client_df, on=\"client_id\", suffixes=[\"_account\", \"_client\"])\n",
    "\n",
    "    # Merge with districts\n",
    "    loan_dfs[i] = loan_dfs[i].merge(district_df, left_on=\"district_id_client\", right_on=\"code\")\n",
    "\n",
    "    # Merge with cards\n",
    "    loan_dfs[i] = loan_dfs[i].merge(cards_dfs[i], on=\"disp_id\", how=\"left\")\n",
    "\n",
    "    # Merge with transactions\n",
    "    loan_dfs[i] = loan_dfs[i].merge(trans_dfs[i], on=\"account_id\", suffixes=[\"_card\", \"_transaction\"])\n",
    "\n",
    "loan_train_df, loan_test_df = loan_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode district name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_train_df[\"status\"] = loan_train_df[\"status\"].apply(lambda x: True if (x == 1) else False)\n",
    "columns = [\"region\"]\n",
    "woe_encoder = ce.WOEEncoder(cols=columns).fit(loan_train_df[columns], loan_train_df[\"status\"])\n",
    "woe_encoded_train = woe_encoder.transform(loan_train_df[columns]).add_suffix('_woe')\n",
    "loan_train_df = loan_train_df.join(woe_encoded_train)\n",
    "loan_train_df[\"status\"] = loan_train_df[\"status\"].apply(lambda x: 1 if (x == True) else -1)\n",
    "\n",
    "loan_test_df[\"status\"] = loan_test_df[\"status\"].apply(lambda x: True if (x == 1) else False)\n",
    "woe_encoded_test = woe_encoder.transform(loan_test_df[columns]).add_suffix('_woe')\n",
    "loan_test_df = loan_test_df.join(woe_encoded_test)\n",
    "loan_test_df[\"status\"] = loan_test_df[\"status\"].apply(lambda x: 1 if (x == True) else -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All cards can be dropped as there are only 11 out of the total 328 loans making it very hard or impossible to fill in missing values.\n",
    "- IDs are no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_dfs = [loan_train_df, loan_test_df]\n",
    "\n",
    "for i in range(len(loan_dfs)):\n",
    "    loan_dfs[i].drop([\"card_id\", \"type_card\", \"issued\"], axis=1, inplace=True)\n",
    "    loan_dfs[i].drop([\"disp_id\", \"account_id\", \"client_id\"], axis=1, inplace=True)\n",
    "    loan_dfs[i].drop([\"district_id_account\", \"district_id_client\"], axis=1, inplace=True)\n",
    "    loan_dfs[i].drop([\"trans_id\"], axis=1, inplace=True)\n",
    "\n",
    "loan_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_dfs = [loan_train_df, loan_test_df]\n",
    "\n",
    "def count_withdrawal(x):\n",
    "    return sum(x==\"withdrawal\")\n",
    "\n",
    "def count_credit(x):\n",
    "    return sum(x==\"credit\")\n",
    "\n",
    "for i in range(len(loan_dfs)):\n",
    "    aggregated_columns = (\"transaction_date\", \"operation\", \"amount\", \"balance\", \"type_transaction\", \"client_count\")\n",
    "    columns = [x for x in loan_dfs[i].columns.to_list() if x not in aggregated_columns]\n",
    "\n",
    "    df = loan_dfs[i].groupby(columns, as_index=False, group_keys=False, dropna=False)\n",
    "\n",
    "    num_times_under_zero = df.apply(lambda x: pd.Series(dict(\n",
    "        num_times_under_zero = (x.balance < x.payments).sum() > 2\n",
    "    )))[\"num_times_under_zero\"]\n",
    "\n",
    "    df = df.agg({\n",
    "        \"balance\": [\"mean\", \"min\", \"max\"],\n",
    "        \"transaction_date\": [\"max\"],\n",
    "        \"client_count\": [\"mean\"],\n",
    "        \"operation\": [\"count\"],\n",
    "        \"amount\": [\"mean\", \"min\", \"max\", \"std\"],\n",
    "        \"type_transaction\": [count_withdrawal, count_credit]\n",
    "    })\n",
    "\n",
    "    df[\"balance_dropped_below_zero\"] = num_times_under_zero\n",
    "\n",
    "    df.columns = ['%s%s' % (a, '_%s' % b if b else '') for a, b in df.columns]\n",
    "\n",
    "    # Account age at time of loan in days\n",
    "    df[\"account_age\"] = (df['loan_date'] - df['creation_date']).dt.days\n",
    "\n",
    "    # Number of days since last transaction\n",
    "    df[\"days_since_last_transaction\"] = (df[\"loan_date\"] - df[\"transaction_date_max\"]).dt.days\n",
    "\n",
    "    # Whether an account has reached a negative balance\n",
    "    df[\"reached_negative_balance\"] = df[\"balance_min\"] < 0\n",
    "\n",
    "    # Drop non-numeric columns\n",
    "    loan_dfs[i] = df.select_dtypes([\"number\", \"bool\"])\n",
    "\n",
    "    # loan_dfs[i].drop([\"balance_min\", \"balance_max\"], axis=1, inplace=True)\n",
    "\n",
    "    status = loan_dfs[i].pop(\"status\")\n",
    "    loan_dfs[i][\"status\"] = status\n",
    "\n",
    "train_df, test_df = loan_dfs\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "RANDOM_STATE=42\n",
    "\n",
    "X = train_df\n",
    "# X = train_df[[\"reached_negative_balance\", \"balance_min\", \"balance_mean\"]]\n",
    "y = train_df[\"status\"]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(X)\n",
    "\n",
    "distorsions = []\n",
    "silhouette = []\n",
    "\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE)\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "    silhouette.append(metrics.silhouette_score(X, kmeans.labels_))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(range(2, 10), silhouette)\n",
    "ax[1].plot(range(2, 10), distorsions)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "ax[0].set_title('Silhouette method')\n",
    "ax[1].set_title('Elbow curve')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = train_df\n",
    "# X = train_df[[\"reached_negative_balance\", \"balance_min\", \"balance_mean\"]]\n",
    "y = train_df[\"status\"]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = PCA(n_components=2, random_state=RANDOM_STATE).fit_transform(X)\n",
    "db = KMeans(n_clusters=3, random_state=RANDOM_STATE).fit_predict(X)\n",
    "\n",
    "# Plot result\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1], c=db)\n",
    "\n",
    "scatter = axs[1].scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "axs[0].set_title(\"Kmeans\")\n",
    "axs[1].set_title(\"Status\")\n",
    "\n",
    "axs[1].legend(handles=scatter.legend_elements()[0], labels=[-1, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Age distribution by loan request\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Number of loans\")\n",
    "\n",
    "sb.histplot(data=loan_train_df, x=\"age\", hue=\"status\", bins=20).set(title=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.histplot(data=loan_train_df, x=loan_train_df[\"loan_amount\"], hue=\"status\", bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "sb.scatterplot(x=loan_train_df[\"average salary\"], y=loan_train_df[\"loan_amount\"], marker=\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = loan_train_df.drop(\"loan_id\", axis=1).corr(method='spearman')\n",
    "\n",
    "threshold = 0.05\n",
    "\n",
    "correlation_status = corr_matrix.loc[['status'], :]\n",
    "selected_cols = set(correlation_status.loc[:, (abs(correlation_status) > threshold).any()].columns.to_list())\n",
    "dropped_cols = set.difference(set(correlation_status.columns.to_list()), selected_cols)\n",
    "\n",
    "loan_train_df.drop(dropped_cols, axis=1, inplace=True)\n",
    "loan_test_df.drop(dropped_cols, axis=1, inplace=True)\n",
    "\n",
    "corr_matrix = loan_train_df.drop(\"loan_id\", axis=1).corr(method='spearman')\n",
    "\n",
    "mask = np.zeros(corr_matrix.shape, dtype=bool)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "mask[np.triu_indices(len(mask))] = True\n",
    "\n",
    "plt.title('Correlation Heatmap of client Dataset')\n",
    "\n",
    "sb.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='black', mask=mask, cbar=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns to drop and feature target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"loan_id\", \"status\"]\n",
    "target_column = \"status\"\n",
    "\n",
    "train_df.drop(columns_to_drop, axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model and combination of applying or not oversampling/feature selection, we plot ROC curves and confusion matrix (in which 0 represents `status` equal to -1, that is, rejected loans). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "parameter_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': range(1, 7)\n",
    "}\n",
    "\n",
    "dt, dt_fs, dt_os, dt_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column,\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([dt, dt_fs, dt_os, dt_fs_os], train_df, columns_to_drop, target_column, scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([dt, dt_fs, dt_os, dt_fs_os], columns_to_drop, target_column, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Needs to be scaled (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "parameter_grid = {\n",
    "    'C': [1, 10, 50],\n",
    "    'gamma': [0.001, 0.0001],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "svc, svc_fs, svc_os, svc_fs_os = (tune_model(\n",
    "    train_df,\n",
    "    SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    parameter_grid,\n",
    "    columns_to_drop,\n",
    "    target_column,\n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True, True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([svc, svc_fs, svc_os, svc_fs_os], train_df, columns_to_drop, target_column, scaler=StandardScaler());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([svc, svc_fs, svc_os, svc_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-nearest neighbours (KNN)\n",
    "Just like the SVM model, the KNN model also requires the data to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_neighbors': [4, 5, 6, 7, 10, 15],\n",
    "    'leaf_size': [5, 10, 15, 20, 50, 100],\n",
    "    'n_jobs': [-1],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "knn, knn_fs, knn_os, knn_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    neighbors.KNeighborsClassifier(), \n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column, \n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([knn, knn_fs, knn_os, knn_fs_os], train_df, columns_to_drop, target_column, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([knn, knn_fs, knn_os, knn_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "parameter_grid = {}\n",
    "\n",
    "nb, nb_fs, nb_os, nb_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    GaussianNB(),\n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column, \n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([nb, nb_fs, nb_os, nb_fs_os], train_df, columns_to_drop, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([nb, nb_fs, nb_os, nb_fs_os], columns_to_drop, target_column, train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(nb.best_estimator_.steps[-1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns, columns=[\"Importance\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'n_jobs': [-1],  # Use all cores\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rfc, rfc_fs, rfc_os, rfc_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    RandomForestClassifier(random_state=RANDOM_STATE), \n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column,\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([rfc, rfc_fs, rfc_os, rfc_fs_os], train_df, columns_to_drop, target_column, scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([rfc, rfc_fs, rfc_os, rfc_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(rfc.best_estimator_.steps[-1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns, columns=[\"Importance\"]))\n",
    "# print(pd.DataFrame(rfc_fs.best_estimator_.steps[1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns, columns=[\"Importance\"]))\n",
    "# print(pd.DataFrame(rfc_fs.best_estimator_.steps[1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns[rfc_fs.best_estimator_.steps[0][1].get_feature_names_out()], columns=[\"Importance\"]))\n",
    "# rfc_fs.best_estimator_.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameter_grid = {\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "    \"solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "lr, lr_fs, lr_os, lr_fs_os = (tune_model(\n",
    "    train_df,\n",
    "    LogisticRegression(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "    parameter_grid,\n",
    "    columns_to_drop,\n",
    "    target_column,\n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([lr, lr_fs, lr_os, lr_fs_os], train_df, columns_to_drop, target_column, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([lr, lr_fs, lr_os, lr_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "scores = {\n",
    "    \"Decision Tree\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"SVC\": [svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"K-nearest Neighbours\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"Naive Bayes\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"Random Forest\": [rfc, rfc_fs, rfc_os, rfc_fs_os],\n",
    "    \"Logistic Regression\": [lr, lr_fs, lr_os, lr_fs_os]\n",
    "}\n",
    "\n",
    "x_axis_labels = [\"No Feature selection/No oversampling\",\n",
    "                 \"Feature Selection\", \"Oversampling\", \"Feature Selection/Oversampling\"]\n",
    "y_axis_labels = scores.keys()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scores_array = np.array([[model.best_score_ for model in models]\n",
    "                         for models in scores.values()])\n",
    "\n",
    "sb.set(font_scale=1.3)\n",
    "sb.heatmap(scores_array, annot=True, linewidths=0.5, vmax=1,\n",
    "           square=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cbar=False)\n",
    "plt.title(\"ROC-AUC\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right')\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "scores = {\n",
    "    \"Decision Tree\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"SVC\": [svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"K-nearest Neighbours\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"Naive Bayes\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"Random Forest\": [rfc, rfc_fs, rfc_os, rfc_fs_os],\n",
    "    \"Logistic Regression\": [lr, lr_fs, lr_os, lr_fs_os]\n",
    "}\n",
    "\n",
    "x_axis_labels = [\"No Feature selection/No oversampling\",\n",
    "                 \"Feature Selection\", \"Oversampling\", \"Feature Selection/Oversampling\"]\n",
    "y_axis_labels = scores.keys()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scale = [None, StandardScaler(), StandardScaler(), StandardScaler(), None, StandardScaler()]\n",
    "\n",
    "scores_array = np.array([[accuracy_score(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[1], \n",
    "                            models[i].best_estimator_.predict(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[0])) for i in range(len(models))]\n",
    "                         for models in scores.values()])\n",
    "\n",
    "sb.set(font_scale=1.3)\n",
    "sb.heatmap(scores_array, annot=True, linewidths=0.5, vmax=1,\n",
    "           square=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cbar=False)\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "scores = {\n",
    "    \"Decision Tree\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"SVC\": [svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"K-nearest Neighbours\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"Naive Bayes\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"Random Forest\": [rfc, rfc_fs, rfc_os, rfc_fs_os],\n",
    "    \"Logistic Regression\": [lr, lr_fs, lr_os, lr_fs_os]\n",
    "}\n",
    "\n",
    "x_axis_labels = [\"No Feature selection/No oversampling\",\n",
    "                 \"Feature Selection\", \"Oversampling\", \"Feature Selection/Oversampling\"]\n",
    "y_axis_labels = scores.keys()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scale = [None, StandardScaler(), StandardScaler(), StandardScaler(), None, StandardScaler()]\n",
    "\n",
    "scores_array = np.array([[f1_score(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[1], \n",
    "                            models[i].best_estimator_.predict(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[0])) for i in range(len(models))]\n",
    "                         for models in scores.values()])\n",
    "\n",
    "sb.set(font_scale=1.3)\n",
    "sb.heatmap(scores_array, annot=True, linewidths=0.5, vmax=1,\n",
    "           square=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cbar=False)\n",
    "plt.title(\"F1-Score\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [False, True, True, True, False, True]\n",
    "\n",
    "plotAlgorithmROC([dt, svc, knn, nb, rfc, lr],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAlgorithmROC([dt_fs, svc_fs, knn_fs, nb_fs, rfc_fs, lr_fs],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAlgorithmROC([dt_os, svc_os, knn_os, nb_os, rfc_os, lr_os],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAlgorithmROC([dt_fs_os, svc_fs_os, knn_fs_os, nb_fs_os, rfc_fs_os, lr_fs_os],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([dt, dt_fs, dt_os, dt_fs_os], columns_to_drop, target_column, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Needs to be scaled (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "parameter_grid = {\n",
    "    'C': [1, 10, 50],\n",
    "    'gamma': [0.001, 0.0001],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "\n",
    "svc, svc_fs, svc_os, svc_fs_os = (tune_model(\n",
    "    train_df,\n",
    "    SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    parameter_grid,\n",
    "    columns_to_drop,\n",
    "    target_column,\n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True, True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([svc, svc_fs, svc_os, svc_fs_os], train_df, columns_to_drop, target_column, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([svc, svc_fs, svc_os, svc_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  K-nearest neighbours (KNN)\n",
    "Just like the SVM model, the KNN model also requires the data to be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_neighbors': [4, 5, 6, 7, 10, 15],\n",
    "    'leaf_size': [5, 10, 15, 20, 50, 100],\n",
    "    'n_jobs': [-1],\n",
    "    'algorithm': ['auto']\n",
    "}\n",
    "\n",
    "knn, knn_fs, knn_os, knn_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    neighbors.KNeighborsClassifier(), \n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column, \n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([knn, knn_fs, knn_os, knn_fs_os], train_df, columns_to_drop, target_column, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([knn, knn_fs, knn_os, knn_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "parameter_grid = {}\n",
    "\n",
    "nb, nb_fs, nb_os, nb_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    GaussianNB(),\n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column, \n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([nb, nb_fs, nb_os, nb_fs_os], train_df, columns_to_drop, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([nb, nb_fs, nb_os, nb_fs_os], columns_to_drop, target_column, train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.DataFrame(nb.best_estimator_.steps[-1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns, columns=[\"Importance\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'n_jobs': [-1],  # Use all cores\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rfc, rfc_fs, rfc_os, rfc_fs_os = (tune_model(\n",
    "    train_df, \n",
    "    RandomForestClassifier(random_state=RANDOM_STATE), \n",
    "    parameter_grid, \n",
    "    columns_to_drop, \n",
    "    target_column,\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([rfc, rfc_fs, rfc_os, rfc_fs_os], train_df, columns_to_drop, target_column, scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([rfc, rfc_fs, rfc_os, rfc_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(rfc.best_estimator_.steps[-1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns, columns=[\"Importance\"]))\n",
    "# print(pd.DataFrame(rfc_fs.best_estimator_.steps[1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns, columns=[\"Importance\"]))\n",
    "# print(pd.DataFrame(rfc_fs.best_estimator_.steps[1][1].feature_importances_, index=train_df.drop(columns_to_drop, axis=1).columns[rfc_fs.best_estimator_.steps[0][1].get_feature_names_out()], columns=[\"Importance\"]))\n",
    "# rfc_fs.best_estimator_.steps[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "parameter_grid = {\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "    \"solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "lr, lr_fs, lr_os, lr_fs_os = (tune_model(\n",
    "    train_df,\n",
    "    LogisticRegression(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "    parameter_grid,\n",
    "    columns_to_drop,\n",
    "    target_column,\n",
    "    scaler=StandardScaler(),\n",
    "    oversample=oversample,\n",
    "    feature_selection=feature_selection\n",
    ") for oversample, feature_selection in ((False, False), (False, True), (True, False), (True,True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC([lr, lr_fs, lr_os, lr_fs_os], train_df, columns_to_drop, target_column, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix([lr, lr_fs, lr_os, lr_fs_os], columns_to_drop, target_column, train_df, scaler=StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "\n",
    "scores = {\n",
    "    \"Decision Tree\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"SVC\": [svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"K-nearest Neighbours\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"Naive Bayes\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"Random Forest\": [rfc, rfc_fs, rfc_os, rfc_fs_os],\n",
    "    \"Logistic Regression\": [lr, lr_fs, lr_os, lr_fs_os]\n",
    "}\n",
    "\n",
    "x_axis_labels = [\"No Feature selection/No oversampling\",\n",
    "                 \"Feature Selection\", \"Oversampling\", \"Feature Selection/Oversampling\"]\n",
    "y_axis_labels = scores.keys()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scores_array = np.array([[model.best_score_ for model in models]\n",
    "                         for models in scores.values()])\n",
    "\n",
    "sb.set(font_scale=1.3)\n",
    "sb.heatmap(scores_array, annot=True, linewidths=0.5, vmax=1,\n",
    "           square=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cbar=False)\n",
    "plt.title(\"ROC-AUC\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "scores = {\n",
    "    \"Decision Tree\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"SVC\": [svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"K-nearest Neighbours\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"Naive Bayes\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"Random Forest\": [rfc, rfc_fs, rfc_os, rfc_fs_os],\n",
    "    \"Logistic Regression\": [lr, lr_fs, lr_os, lr_fs_os]\n",
    "}\n",
    "\n",
    "x_axis_labels = [\"No Feature selection/No oversampling\",\n",
    "                 \"Feature Selection\", \"Oversampling\", \"Feature Selection/Oversampling\"]\n",
    "y_axis_labels = scores.keys()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scale = [None, StandardScaler(), StandardScaler(), StandardScaler(), None, StandardScaler()]\n",
    "\n",
    "scores_array = np.array([[accuracy_score(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[1], \n",
    "                            models[i].best_estimator_.predict(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[0])) for i in range(len(models))]\n",
    "                         for models in scores.values()])\n",
    "\n",
    "sb.set(font_scale=1.3)\n",
    "sb.heatmap(scores_array, annot=True, linewidths=0.5, vmax=1,\n",
    "           square=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cbar=False)\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "scores = {\n",
    "    \"Decision Tree\": [dt, dt_fs, dt_os, dt_fs_os],\n",
    "    \"SVC\": [svc, svc_fs, svc_os, svc_fs_os],\n",
    "    \"K-nearest Neighbours\": [knn, knn_fs, knn_os, knn_fs_os],\n",
    "    \"Naive Bayes\": [nb, nb_fs, nb_os, nb_fs_os],\n",
    "    \"Random Forest\": [rfc, rfc_fs, rfc_os, rfc_fs_os],\n",
    "    \"Logistic Regression\": [lr, lr_fs, lr_os, lr_fs_os]\n",
    "}\n",
    "\n",
    "x_axis_labels = [\"No Feature selection/No oversampling\",\n",
    "                 \"Feature Selection\", \"Oversampling\", \"Feature Selection/Oversampling\"]\n",
    "y_axis_labels = scores.keys()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "scale = [None, StandardScaler(), StandardScaler(), StandardScaler(), None, StandardScaler()]\n",
    "\n",
    "scores_array = np.array([[f1_score(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[1], \n",
    "                            models[i].best_estimator_.predict(get_X_y(train_df, columns_to_drop, target_column, scaler=scale[i])[0])) for i in range(len(models))]\n",
    "                         for models in scores.values()])\n",
    "\n",
    "sb.set(font_scale=1.3)\n",
    "sb.heatmap(scores_array, annot=True, linewidths=0.5, vmax=1,\n",
    "           square=True, xticklabels=x_axis_labels, yticklabels=y_axis_labels, cbar=False)\n",
    "plt.title(\"F1-Score\")\n",
    "\n",
    "plt.xticks(rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [False, True, True, True, False, True]\n",
    "\n",
    "plotAlgorithmROC([dt, svc, knn, nb, rfc, lr],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAlgorithmROC([dt_fs, svc_fs, knn_fs, nb_fs, rfc_fs, lr_fs],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAlgorithmROC([dt_os, svc_os, knn_os, nb_os, rfc_os, lr_os],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection / Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAlgorithmROC([dt_fs_os, svc_fs_os, knn_fs_os, nb_fs_os, rfc_fs_os, lr_fs_os],\n",
    "                 [\"Decision Tree\", \"SVC\", \"KNN\", \"Naive Bayes\", \"Random Forest\", \"Logistic Regression\"],\n",
    "                 train_df,\n",
    "                 columns_to_drop,\n",
    "                 target_column,\n",
    "                 scalers=scalers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_result = test_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "y_result = dt_fs_os.predict_proba(X_result)[:, 0]\n",
    "\n",
    "result = pd.DataFrame({\"Id\": test_df[\"loan_id\"], \"Predicted\": y_result})\n",
    "result.drop_duplicates(inplace=True)\n",
    "result.to_csv(\"../out/result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
